{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "First, we import the necessary libraries. We'll use TensorFlow and Keras for the model, `os` and `glob` for file handling, and OpenCV (`cv2`) for image processing and drawing bounding boxes during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:26:14.568768: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-26 15:26:15.262610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-26 15:26:18.020906: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set up the main configuration variables here. You should adjust `NUM_CLASSES` based on your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (based on your image)\n",
    "TRAIN_IMAGE_DIR = 'archive/images/train/'\n",
    "TRAIN_LABEL_DIR = 'archive/labels/train/'\n",
    "VAL_IMAGE_DIR = 'archive/images/val/'\n",
    "VAL_LABEL_DIR = 'archive/labels/val/'\n",
    "\n",
    "# Model and image parameters\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 6\n",
    "NUM_CLASSES = 10 # IMPORTANT: Change this to the number of classes in your dataset\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "This function will parse the image and label directories to create a dataset. It reads the YOLO-style `.txt` files to get the bounding box coordinates and class labels. We then create a `tf.data.Dataset` for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756202198.414375    1418 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1756202198.566878    1418 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def parse_label_file(label_path):\n",
    "    \"\"\"Reads a YOLO-style label file and returns the class_id and bbox.\"\"\"\n",
    "    with open(label_path, 'r') as f:\n",
    "        # Assuming one object per file for this simple model\n",
    "        line = f.readline().strip().split()\n",
    "        class_id = int(line[0])\n",
    "        coords = np.array([float(x) for x in line[1:]], dtype=np.float32)\n",
    "    return class_id, coords\n",
    "\n",
    "def load_data(image_dir, label_dir):\n",
    "    \"\"\"Loads image paths and their corresponding labels.\"\"\"\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.jpg'))) # Assuming .jpg, change if needed\n",
    "    label_paths = sorted(glob.glob(os.path.join(label_dir, '*.txt')))\n",
    "    \n",
    "    labels = []\n",
    "    for path in label_paths:\n",
    "        class_id, bbox = parse_label_file(path)\n",
    "        # One-hot encode the class ID\n",
    "        one_hot_class = tf.keras.utils.to_categorical(class_id, num_classes=NUM_CLASSES)\n",
    "        labels.append({'class': one_hot_class, 'bbox': bbox})\n",
    "        \n",
    "    return image_paths, labels\n",
    "\n",
    "def data_generator(image_paths, labels):\n",
    "    \"\"\"A generator to load and preprocess images and labels.\"\"\"\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        # Load and resize image\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        yield img, (label['bbox'], label['class'])\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_images, train_labels = load_data(TRAIN_IMAGE_DIR, TRAIN_LABEL_DIR)\n",
    "val_images, val_labels = load_data(VAL_IMAGE_DIR, VAL_LABEL_DIR)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_images, train_labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
    "        (tf.TensorSpec(shape=(4,), dtype=tf.float32), tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32))\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_images, val_labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
    "        (tf.TensorSpec(shape=(4,), dtype=tf.float32), tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32))\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modified Object Detection Model\n",
    "Here, we adapt your original `CNN_model`. The main change is at the end. Instead of one output layer, we create two separate 'heads':\n",
    "1.  **`bbox_head`**: Predicts the 4 bounding box coordinates.\n",
    "2.  **`class_head`**: Predicts the class of the object, just like in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjectDetector_model(num_classes, input_shape=(512, 512, 3)):\n",
    "    # Base feature extractor (your original model)\n",
    "    base_model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "\n",
    "        keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "    ], name='feature_extractor')\n",
    "\n",
    "    # Common dense layers\n",
    "    x = keras.layers.Dense(512, activation='relu')(base_model.output)\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Bounding Box Head - predicts coordinates (x_center, y_center, width, height)\n",
    "    bbox_head = keras.layers.Dense(4, activation='sigmoid', name='bbox')(x)\n",
    "\n",
    "    # Classification Head - predicts the object class\n",
    "    class_head = keras.layers.Dense(num_classes, activation='softmax', name='class')(x)\n",
    "\n",
    "    # Combine into the final model\n",
    "    model = keras.Model(inputs=base_model.input, outputs=[bbox_head, class_head])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = ObjectDetector_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compiling and Training the Model\n",
    "We compile the model with two different loss functions: `MeanSquaredError` for the bounding box regression and `CategoricalCrossentropy` for the classification. We can also assign different weights to each loss.\n",
    "Then, we train the model using `model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses for each output head\n",
    "losses = {\n",
    "    'bbox': tf.keras.losses.MeanSquaredError(),\n",
    "    'class': tf.keras.losses.CategoricalCrossentropy()\n",
    "}\n",
    "\n",
    "# Define weights for each loss\n",
    "loss_weights = {\n",
    "    'bbox': 1.0,\n",
    "    'class': 1.0\n",
    "}\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving the Model\n",
    "After training, we save the model in TensorFlow's standard `SavedModel` format. We also convert and save it as an `.onnx` file, as requested. You'll need to install the `tf2onnx` library for this step:\n",
    "\n",
    "`pip install tf2onnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in TensorFlow SavedModel format\n",
    "TF_MODEL_PATH = 'object_detector_model_tf'\n",
    "model.save(TF_MODEL_PATH)\n",
    "print(f\"Model saved in TensorFlow format at: {TF_MODEL_PATH}\")\n",
    "\n",
    "# Save in ONNX format\n",
    "try:\n",
    "    import tf2onnx\n",
    "    ONNX_MODEL_PATH = 'object_detector_model.onnx'\n",
    "    spec = (tf.TensorSpec((None, IMG_SIZE, IMG_SIZE, 3), tf.float32, name=\"input\"),)\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=ONNX_MODEL_PATH)\n",
    "    print(f\"Model saved in ONNX format at: {ONNX_MODEL_PATH}\")\n",
    "except ImportError:\n",
    "    print(\"Could not save in ONNX format. Please run 'pip install tf2onnx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference and Visualization\n",
    "Finally, we'll create an inference function to test our trained model. This function will:\n",
    "1. Load the saved TensorFlow model.\n",
    "2. Preprocess a test image.\n",
    "3. Get predictions from the model.\n",
    "4. Convert the normalized coordinates back to pixel values.\n",
    "5. Draw the bounding box and class label on the image using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_path, image_path, class_names):\n",
    "    \"\"\"Loads a model and an image, performs inference, and draws the bounding box.\"\"\"\n",
    "    # Load the model\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    original_h, original_w, _ = image_bgr.shape\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "    image_normalized = image_resized / 255.0\n",
    "    image_batch = np.expand_dims(image_normalized, axis=0) # Add batch dimension\n",
    "    \n",
    "    # Make predictions\n",
    "    bbox_pred, class_pred = loaded_model.predict(image_batch)\n",
    "    \n",
    "    # Post-process the output\n",
    "    bbox = bbox_pred[0] # Get the first (and only) prediction\n",
    "    x_center, y_center, w, h = bbox\n",
    "    \n",
    "    # Denormalize coordinates\n",
    "    x_min = int((x_center - w / 2) * original_w)\n",
    "    y_min = int((y_center - h / 2) * original_h)\n",
    "    x_max = int((x_center + w / 2) * original_w)\n",
    "    y_max = int((y_center + h / 2) * original_h)\n",
    "    \n",
    "    # Find the predicted class\n",
    "    predicted_class_id = np.argmax(class_pred[0])\n",
    "    confidence = np.max(class_pred[0])\n",
    "    class_label = class_names.get(predicted_class_id, 'Unknown')\n",
    "    label_text = f'{class_label}: {confidence:.2f}'\n",
    "\n",
    "    # Draw bounding box and label on the original image\n",
    "    cv2.rectangle(image_bgr, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    cv2.putText(image_bgr, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the result\n",
    "    plt.imshow(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Inference Result')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Create a dummy dictionary for class names. Replace with your actual class names.\n",
    "CLASS_NAMES = {0: 'cat', 1: 'dog', 2: 'car'} # etc. \n",
    "\n",
    "# Get a few test images from the validation set\n",
    "test_image_paths = val_images[:3] \n",
    "\n",
    "for img_path in test_image_paths:\n",
    "    print(f\"Running inference on: {img_path}\")\n",
    "    run_inference(TF_MODEL_PATH, img_path, CLASS_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcnn (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
